{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced Parquet Analysis Queries\n",
        "\n",
        "This notebook demonstrates advanced queries for analyzing the Parquet files using DuckDB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "import duckdb\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configure visualization settings\n",
        "plt.style.use('bmh')\n",
        "sns.set_theme()\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Initialize DuckDB with optimized settings\n",
        "conn = duckdb.connect(database=':memory:', read_only=False)\n",
        "conn.execute(\"SET memory_limit='28GB'\")\n",
        "conn.execute(\"PRAGMA threads=8\")\n",
        "\n",
        "print(\"âœ… DuckDB configured for analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Source Record Analysis\n",
        "\n",
        "Analyzing the distribution of source record types:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "source_query = \"\"\"\n",
        "WITH source_types AS (\n",
        " SELECT \n",
        " json_extract_string(source.value, '$.source_type') as source_type\n",
        " FROM read_parquet('/data/parquet/*.parquet'),\n",
        " UNNEST(json_extract_array(json, '$._source.source_records')) AS source\n",
        ")\n",
        "SELECT \n",
        " source_type,\n",
        " COUNT(*) as count,\n",
        " ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as percentage\n",
        "FROM source_types\n",
        "GROUP BY source_type\n",
        "ORDER BY count DESC;\n",
        "\"\"\"\n",
        "\n",
        "df_sources = conn.execute(source_query).fetchdf()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.pie(df_sources['percentage'], labels=df_sources['source_type'], autopct='%1.1f%%')\n",
        "plt.title('Distribution of Source Record Types')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Cover URL Analysis\n",
        "\n",
        "Analyzing cover URL patterns and sources:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "cover_query = \"\"\"\n",
        "WITH cover_sources AS (\n",
        " SELECT \n",
        " CASE\n",
        " WHEN json_extract_string(json, '$._source.file_unified_data.cover_url_best') LIKE '%archive.org%' THEN 'Internet Archive'\n",
        " WHEN json_extract_string(json, '$._source.file_unified_data.cover_url_best') LIKE '%openlibrary.org%' THEN 'Open Library'\n",
        " WHEN json_extract_string(json, '$._source.file_unified_data.cover_url_best') LIKE '%isbndb.com%' THEN 'ISBNdb'\n",
        " ELSE 'Other'\n",
        " END as cover_source,\n",
        " COUNT(*) as count\n",
        " FROM read_parquet('/data/parquet/*.parquet')\n",
        " WHERE json_extract_string(json, '$._source.file_unified_data.cover_url_best') IS NOT NULL\n",
        " GROUP BY cover_source\n",
        ")\n",
        "SELECT * FROM cover_sources\n",
        "ORDER BY count DESC;\n",
        "\"\"\"\n",
        "\n",
        "df_covers = conn.execute(cover_query).fetchdf()\n",
        "display(df_covers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Added Date Analysis\n",
        "\n",
        "Analyzing when records were added across different sources:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "dates_query = \"\"\"\n",
        "WITH dates AS (\n",
        " SELECT\n",
        " json_extract_string(json, '$._source.file_unified_data.added_date_unified.date_ia_source') as ia_date,\n",
        " json_extract_string(json, '$._source.file_unified_data.added_date_unified.date_ol_source') as ol_date,\n",
        " json_extract_string(json, '$._source.file_unified_data.added_date_unified.date_isbndb_scrape') as isbndb_date\n",
        " FROM read_parquet('/data/parquet/*.parquet')\n",
        " WHERE json_extract_string(json, '$._source.file_unified_data.added_date_unified') IS NOT NULL\n",
        ")\n",
        "SELECT \n",
        " 'Internet Archive' as source,\n",
        " COUNT(*) as total_records,\n",
        " MIN(ia_date) as earliest_date,\n",
        " MAX(ia_date) as latest_date\n",
        "FROM dates WHERE ia_date IS NOT NULL\n",
        "UNION ALL\n",
        "SELECT \n",
        " 'Open Library' as source,\n",
        " COUNT(*) as total_records,\n",
        " MIN(ol_date) as earliest_date,\n",
        " MAX(ol_date) as latest_date\n",
        "FROM dates WHERE ol_date IS NOT NULL\n",
        "UNION ALL\n",
        "SELECT \n",
        " 'ISBNdb' as source,\n",
        " COUNT(*) as total_records,\n",
        " MIN(isbndb_date) as earliest_date,\n",
        " MAX(isbndb_date) as latest_date\n",
        "FROM dates WHERE isbndb_date IS NOT NULL;\n",
        "\"\"\"\n",
        "\n",
        "df_dates = conn.execute(dates_query).fetchdf()\n",
        "display(df_dates)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Language and Content Type Analysis\n",
        "\n",
        "Analyzing language distribution and content types:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "lang_content_query = \"\"\"\n",
        "WITH stats AS (\n",
        " SELECT\n",
        " json_extract_string(json, '$._source.file_unified_data.content_type_best') as content_type,\n",
        " UNNEST(json_extract_array(json, '$._source.file_unified_data.language_codes')) as language\n",
        " FROM read_parquet('/data/parquet/*.parquet')\n",
        " WHERE json_extract_string(json, '$._source.file_unified_data.content_type_best') IS NOT NULL\n",
        ")\n",
        "SELECT \n",
        " content_type,\n",
        " language,\n",
        " COUNT(*) as count\n",
        "FROM stats\n",
        "GROUP BY content_type, language\n",
        "HAVING count > 1000\n",
        "ORDER BY count DESC;\n",
        "\"\"\"\n",
        "\n",
        "df_lang_content = conn.execute(lang_content_query).fetchdf()\n",
        "\n",
        "# Create a pivot table\n",
        "pivot_table = df_lang_content.pivot(index='content_type', columns='language', values='count').fillna(0)\n",
        "\n",
        "plt.figure(figsize=(15, 8))\n",
        "sns.heatmap(pivot_table, annot=True, fmt='.0f', cmap='YlOrRd')\n",
        "plt.title('Content Type vs Language Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Identifier Analysis\n",
        "\n",
        "Analyzing the distribution of different identifiers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "identifier_query = \"\"\"\n",
        "SELECT\n",
        " 'ISBN-13' as id_type,\n",
        " COUNT(DISTINCT json_extract_string(json, '$._source.file_unified_data.identifiers_unified.isbn13[0]')) as unique_count\n",
        "FROM read_parquet('/data/parquet/*.parquet')\n",
        "UNION ALL\n",
        "SELECT\n",
        " 'OCLC' as id_type,\n",
        " COUNT(DISTINCT json_extract_string(json, '$._source.file_unified_data.identifiers_unified.oclc[0]')) as unique_count\n",
        "FROM read_parquet('/data/parquet/*.parquet')\n",
        "UNION ALL\n",
        "SELECT\n",
        " 'Open Library' as id_type,\n",
        " COUNT(DISTINCT json_extract_string(json, '$._source.file_unified_data.identifiers_unified.ol[0]')) as unique_count\n",
        "FROM read_parquet('/data/parquet/*.parquet');\n",
        "\"\"\"\n",
        "\n",
        "df_identifiers = conn.execute(identifier_query).fetchdf()\n",
        "display(df_identifiers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. File Size Distribution by Content Type\n",
        "\n",
        "Analyzing file sizes across different content types:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "filesize_query = \"\"\"\n",
        "WITH size_stats AS (\n",
        " SELECT\n",
        " json_extract_string(json, '$._source.file_unified_data.content_type_best') as content_type,\n",
        " TRY_CAST(json_extract_string(json, '$._source.file_unified_data.filesize_best') AS FLOAT) / (1024*1024) as size_mb\n",
        " FROM read_parquet('/data/parquet/*.parquet')\n",
        " WHERE json_extract_string(json, '$._source.file_unified_data.filesize_best') IS NOT NULL\n",
        ")\n",
        "SELECT\n",
        " content_type,\n",
        " COUNT(*) as count,\n",
        " ROUND(AVG(size_mb), 2) as avg_size_mb,\n",
        " ROUND(MIN(size_mb), 2) as min_size_mb,\n",
        " ROUND(MAX(size_mb), 2) as max_size_mb\n",
        "FROM size_stats\n",
        "GROUP BY content_type\n",
        "HAVING count > 1000\n",
        "ORDER BY avg_size_mb DESC;\n",
        "\"\"\"\n",
        "\n",
        "df_filesizes = conn.execute(filesize_query).fetchdf()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(df_filesizes['content_type'], df_filesizes['avg_size_mb'])\n",
        "plt.title('Average File Size by Content Type')\n",
        "plt.xlabel('Content Type')\n",
        "plt.ylabel('Average Size (MB)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Classification Analysis\n",
        "\n",
        "Analyzing the unified classifications:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "classification_query = \"\"\"\n",
        "WITH classifications AS (\n",
        " SELECT \n",
        " json_extract_string(json, '$._source.file_unified_data.classifications_unified.ia_collection[0]') as collection,\n",
        " json_extract_string(json, '$._source.file_unified_data.classifications_unified.oclc_holdings[0]') as holdings,\n",
        " json_extract_string(json, '$._source.file_unified_data.classifications_unified.content_type[0]') as content_type\n",
        " FROM read_parquet('/data/parquet/*.parquet')\n",
        ")\n",
        "SELECT \n",
        " collection,\n",
        " holdings,\n",
        " content_type,\n",
        " COUNT(*) as count\n",
        "FROM classifications\n",
        "WHERE collection IS NOT NULL\n",
        "GROUP BY collection, holdings, content_type\n",
        "HAVING count > 100\n",
        "ORDER BY count DESC;\n",
        "\"\"\"\n",
        "\n",
        "df_classifications = conn.execute(classification_query).fetchdf()\n",
        "display(df_classifications)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Search Access Types Analysis\n",
        "\n",
        "Analyzing the distribution of access types:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "access_query = \"\"\"\n",
        "WITH access_types AS (\n",
        " SELECT \n",
        " UNNEST(json_extract_array(json, '$._source.search_only_fields.search_access_types')) as access_type\n",
        " FROM read_parquet('/data/parquet/*.parquet')\n",
        ")\n",
        "SELECT \n",
        " access_type,\n",
        " COUNT(*) as count,\n",
        " ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as percentage\n",
        "FROM access_types\n",
        "GROUP BY access_type\n",
        "ORDER BY count DESC;\n",
        "\"\"\"\n",
        "\n",
        "df_access = conn.execute(access_query).fetchdf()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(df_access['access_type'], df_access['percentage'])\n",
        "plt.title('Distribution of Access Types')\n",
        "plt.xlabel('Access Type')\n",
        "plt.ylabel('Percentage')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Problems Analysis\n",
        "\n",
        "Analyzing reported problems in the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "problems_query = \"\"\"\n",
        "WITH problem_records AS (\n",
        " SELECT \n",
        " UNNEST(json_extract_array(json, '$._source.file_unified_data.problems')) as problem\n",
        " FROM read_parquet('/data/parquet/*.parquet')\n",
        " WHERE json_extract_array(json, '$._source.file_unified_data.problems') IS NOT NULL\n",
        ")\n",
        "SELECT \n",
        " problem,\n",
        " COUNT(*) as count\n",
        "FROM problem_records\n",
        "GROUP BY problem\n",
        "ORDER BY count DESC;\n",
        "\"\"\"\n",
        "\n",
        "df_problems = conn.execute(problems_query).fetchdf()\n",
        "display(df_problems)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Score Analysis\n",
        "\n",
        "Analyzing base rank scores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "score_query = \"\"\"\n",
        "WITH score_stats AS (\n",
        " SELECT\n",
        " json_extract_string(json, '$._source.search_only_fields.search_score_base_rank') as base_rank,\n",
        " json_extract_string(json, '$._source.file_unified_data.content_type_best') as content_type\n",
        " FROM read_parquet('/data/parquet/*.parquet')\n",
        " WHERE json_extract_string(json, '$._source.search_only_fields.search_score_base_rank') IS NOT NULL\n",
        ")\n",
        "SELECT\n",
        " content_type,\n",
        " COUNT(*) as count,\n",
        " ROUND(AVG(CAST(base_rank AS FLOAT)), 2) as avg_rank,\n",
        " MIN(CAST(base_rank AS FLOAT)) as min_rank,\n",
        " MAX(CAST(base_rank AS FLOAT)) as max_rank\n",
        "FROM score_stats\n",
        "GROUP BY content_type\n",
        "HAVING count > 100\n",
        "ORDER BY avg_rank DESC;\n",
        "\"\"\"\n",
        "\n",
        "df_scores = conn.execute(score_query).fetchdf()\n",
        "display(df_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Title Length Analysis\n",
        "\n",
        "Analyzing the distribution of title lengths:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "title_length_query = \"\"\"\n",
        "WITH title_stats AS (\n",
        " SELECT\n",
        " LENGTH(json_extract_string(json, '$._source.file_unified_data.title_best')) as title_length,\n",
        " json_extract_string(json, '$._source.file_unified_data.content_type_best') as content_type\n",
        " FROM read_parquet('/data/parquet/*.parquet')\n",
        " WHERE json_extract_string(json, '$._source.file_unified_data.title_best') IS NOT NULL\n",
        ")\n",
        "SELECT\n",
        " content_type,\n",
        " COUNT(*) as count,\n",
        " ROUND(AVG(title_length), 2) as avg_length,\n",
        " MIN(title_length) as min_length,\n",
        " MAX(title_length) as max_length\n",
        "FROM title_stats\n",
        "GROUP BY content_type\n",
        "HAVING count > 100\n",
        "ORDER BY avg_length DESC;\n",
        "\"\"\"\n",
        "\n",
        "df_titles = conn.execute(title_length_query).fetchdf()\n",
        "display(df_titles)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Additional Filenames Analysis\n",
        "\n",
        "Analyzing patterns in additional filenames:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "filename_query = \"\"\"\n",
        "WITH filename_patterns AS (\n",
        " SELECT\n",
        " json_extract_string(json, '$._source.file_unified_data.original_filename_best') as main_filename,\n",
        " UNNEST(json_extract_array(json, '$._source.file_unified_data.original_filename_additional')) as additional_filename\n",
        " FROM read_parquet('/data/parquet/*.parquet')\n",
        " WHERE json_extract_array(json, '$._source.file_unified_data.original_filename_additional') IS NOT NULL\n",
        ")\n",
        "SELECT\n",
        " COUNT(DISTINCT main_filename) as unique_main_files,\n",
        " COUNT(DISTINCT additional_filename) as unique_additional_files,\n",
        " COUNT(*) as total_relationships\n",
        "FROM filename_patterns;\n",
        "\"\"\"\n",
        "\n",
        "df_filenames = conn.execute(filename_query).fetchdf()\n",
        "display(df_filenames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Author Name Variations\n",
        "\n",
        "Analyzing variations in author names:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "author_variations_query = \"\"\"\n",
        "WITH author_vars AS (\n",
        " SELECT\n",
        " json_extract_string(json, '$._source.file_unified_data.author_best') as main_author,\n",
        " UNNEST(json_extract_array(json, '$._source.file_unified_data.author_additional')) as additional_author\n",
        " FROM read_parquet('/data/parquet/*.parquet')\n",
        " WHERE json_extract_array(json, '$._source.file_unified_data.author_additional') IS NOT NULL\n",
        ")\n",
        "SELECT\n",
        " main_author,\n",
        " COUNT(DISTINCT additional_author) as variation_count,\n",
        " array_agg(DISTINCT additional_author) as variations\n",
        "FROM author_vars\n",
        "GROUP BY main_author\n",
        "HAVING variation_count > 1\n",
        "ORDER BY variation_count DESC\n",
        "LIMIT 10;\n",
        "\"\"\"\n",
        "\n",
        "df_author_vars = conn.execute(author_variations_query).fetchdf()\n",
        "display(df_author_vars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Publisher Evolution\n",
        "\n",
        "Analyzing publisher changes over time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "publisher_evolution_query = \"\"\"\n",
        "WITH pub_timeline AS (\n",
        " SELECT\n",
        " json_extract_string(json, '$._source.file_unified_data.publisher_best') as publisher,\n",
        " TRY_CAST(json_extract_string(json, '$._source.file_unified_data.year_best') AS INTEGER) as year,\n",
        " COUNT(*) as publications\n",
        " FROM read_parquet('/data/parquet/*.parquet')\n",
        " WHERE year BETWEEN 1950 AND 2024\n",
        " GROUP BY publisher, year\n",
        ")\n",
        "SELECT\n",
        " publisher,\n",
        " MIN(year) as first_publication,\n",
        " MAX(year) as last_publication,\n",
        " SUM(publications) as total_publications,\n",
        " COUNT(DISTINCT year) as active_years\n",
        "FROM pub_timeline\n",
        "WHERE publisher IS NOT NULL\n",
        "GROUP BY publisher\n",
        "HAVING total_publications > 1000\n",
        "ORDER BY total_publications DESC;\n",
        "\"\"\"\n",
        "\n",
        "df_pub_evolution = conn.execute(publisher_evolution_query).fetchdf()\n",
        "display(df_pub_evolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Description Analysis\n",
        "\n",
        "Analyzing book descriptions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "description_query = \"\"\"\n",
        "WITH desc_stats AS (\n",
        " SELECT\n",
        " json_extract_string(json, '$._source.file_unified_data.stripped_description_best') as description,\n",
        " json_extract_string(json, '$._source.file_unified_data.content_type_best') as content_type\n",
        " FROM read_parquet('/data/parquet/*.parquet')\n",
        " WHERE json_extract_string(json, '$._source.file_unified_data.stripped_description_best') IS NOT NULL\n",
        ")\n",
        "SELECT\n",
        " content_type,\n",
        " COUNT(*) as total_books,\n",
        " ROUND(AVG(LENGTH(description)), 2) as avg_desc_length,\n",
        " MIN(LENGTH(description)) as min_length,\n",
        " MAX(LENGTH(description)) as max_length\n",
        "FROM desc_stats\n",
        "GROUP BY content_type\n",
        "HAVING total_books > 100\n",
        "ORDER BY avg_desc_length DESC;\n",
        "\"\"\"\n",
        "\n",
        "df_descriptions = conn.execute(description_query).fetchdf()\n",
        "display(df_descriptions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16. Edition Analysis\n",
        "\n",
        "Analyzing edition variations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "edition_query = \"\"\"\n",
        "WITH edition_data AS (\n",
        " SELECT\n",
        " json_extract_string(json, '$._source.file_unified_data.edition_varia_best') as main_edition,\n",
        " UNNEST(json_extract_array(json, '$._source.file_unified_data.edition_varia_additional')) as additional_edition\n",
        " FROM read_parquet('/data/parquet/*.parquet')\n",
        " WHERE json_extract_string(json, '$._source.file_unified_data.edition_varia_best') IS NOT NULL\n",
        ")\n",
        "SELECT\n",
        " main_edition,\n",
        " COUNT(DISTINCT additional_edition) as edition_variations,\n",
        " array_agg(DISTINCT additional_edition) as variations\n",
        "FROM edition_data\n",
        "GROUP BY main_edition\n",
        "HAVING edition_variations > 1\n",
        "ORDER BY edition_variations DESC\n",
        "LIMIT 10;\n",
        "\"\"\"\n",
        "\n",
        "df_editions = conn.execute(edition_query).fetchdf()\n",
        "display(df_editions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 17. AACID Analysis\n",
        "\n",
        "Analyzing AACID patterns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "aacid_query = \"\"\"\n",
        "WITH aacid_patterns AS (\n",
        " SELECT\n",
        " UNNEST(json_extract_array(json, '$._source.file_unified_data.identifiers_unified.aacid')) as aacid,\n",
        " SPLIT_PART(aacid, '__', 2) as source_type,\n",
        " SPLIT_PART(aacid, '__', 3) as timestamp\n",
        " FROM read_parquet('/data/parquet/*.parquet')\n",
        ")\n",
        "SELECT\n",
        " source_type,\n",
        " COUNT(*) as count,\n",
        " MIN(timestamp) as earliest_record,\n",
        " MAX(timestamp) as latest_record\n",
        "FROM aacid_patterns\n",
        "GROUP BY source_type\n",
        "ORDER BY count DESC;\n",
        "\"\"\"\n",
        "\n",
        "df_aacids = conn.execute(aacid_query).fetchdf()\n",
        "display(df_aacids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 18. Search Text Analysis\n",
        "\n",
        "Analyzing search text patterns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "search_text_query = \"\"\"\n",
        "WITH text_stats AS (\n",
        " SELECT\n",
        " json_extract_string(json, '$._source.search_only_fields.search_text') as search_text,\n",
        " json_extract_string(json, '$._source.file_unified_data.content_type_best') as content_type\n",
        " FROM read_parquet('/data/parquet/*.parquet')\n",
        " WHERE json_extract_string(json, '$._source.search_only_fields.search_text') IS NOT NULL\n",
        ")\n",
        "SELECT\n",
        " content_type,\n",
        " COUNT(*) as total_records,\n",
        " ROUND(AVG(LENGTH(search_text)), 2) as avg_text_length,\n",
        " MIN(LENGTH(search_text)) as min_length,\n",
        " MAX(LENGTH(search_text)) as max_length\n",
        "FROM text_stats\n",
        "GROUP BY content_type\n",
        "HAVING total_records > 100\n",
        "ORDER BY avg_text_length DESC;\n",
        "\"\"\"\n",
        "\n",
        "df_search_text = conn.execute(search_text_query).fetchdf()\n",
        "display(df_search_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 19. Download Availability Analysis\n",
        "\n",
        "Analyzing download options:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "download_query = \"\"\"\n",
        "SELECT\n",
        " json_extract_string(json, '$._source.file_unified_data.content_type_best') as content_type,\n",
        " COUNT(*) as total_records,\n",
        " SUM(CASE WHEN json_extract(json, '$._source.file_unified_data.has_aa_downloads') = 'true' THEN 1 ELSE 0 END) as aa_downloads,\n",
        " SUM(CASE WHEN json_extract(json, '$._source.file_unified_data.has_aa_exclusive_downloads') = 'true' THEN 1 ELSE 0 END) as exclusive_downloads,\n",
        " SUM(CASE WHEN json_extract(json, '$._source.file_unified_data.has_torrent_paths') = 'true' THEN 1 ELSE 0 END) as torrent_available,\n",
        " SUM(CASE WHEN json_extract(json, '$._source.file_unified_data.has_scidb') = 'true' THEN 1 ELSE 0 END) as scidb_available\n",
        "FROM read_parquet('/data/parquet/*.parquet')\n",
        "GROUP BY content_type\n",
        "HAVING total_records > 100\n",
        "ORDER BY total_records DESC;\n",
        "\"\"\"\n",
        "\n",
        "df_downloads = conn.execute(download_query).fetchdf()\n",
        "display(df_downloads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 20. Source Record Chain Analysis\n",
        "\n",
        "Analyzing source record relationships:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "source_chain_query = \"\"\"\n",
        "WITH RECURSIVE source_chain AS (\n",
        " SELECT\n",
        " json_extract_string(source.value, '$.source_type') as source_type,\n",
        " json_extract_string(source.value, '$.source_why') as source_why,\n",
        " 1 as chain_depth\n",
        " FROM read_parquet('/data/parquet/*.parquet'),\n",
        " UNNEST(json_extract_array(json, '$._source.source_records')) AS source\n",
        " WHERE json_extract_string(source.value, '$.source_why') NOT LIKE '%transitive%'\n",
        " \n",
        " UNION ALL\n",
        " \n",
        " SELECT\n",
        " json_extract_string(source.value, '$.source_type') as source_type,\n",
        " json_extract_string(source.value, '$.source_why') as source_why,\n",
        " sc.chain_depth + 1\n",
        " FROM read_parquet('/data/parquet/*.parquet'),\n",
        " UNNEST(json_extract_array(json, '$._source.source_records')) AS source,\n",
        " source_chain sc\n",
        " WHERE json_extract_string(source.value, '$.source_why') LIKE '%' || sc.source_type || '%'\n",
        " AND chain_depth < 5\n",
        ")\n",
        "SELECT\n",
        " chain_depth,\n",
        " source_type,\n",
        " COUNT(*) as occurrences\n",
        "FROM source_chain\n",
        "GROUP BY chain_depth, source_type\n",
        "ORDER BY chain_depth, occurrences DESC;\n",
        "\"\"\"\n",
        "\n",
        "df_source_chain = conn.execute(source_chain_query).fetchdf()\n",
        "display(df_source_chain)\n",
        "\n",
        "# Visualize the chain depth distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=df_source_chain, x='chain_depth', y='occurrences', hue='source_type')\n",
        "plt.title('Source Record Chain Depth Distribution')\n",
        "plt.xlabel('Chain Depth')\n",
        "plt.ylabel('Number of Occurrences')\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
